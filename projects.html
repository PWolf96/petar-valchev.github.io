<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<title>PROJECTS</title>
		<style>
			body:before {
				font-family: Arial, sans-serif;
				margin: 0;
				padding: 0;
				background-color: #f2f2f2;
			}

			header::before {
				background-color: #333;
				color: #fff;
				padding: 20px;
				text-align: center;
			}

			h1 {
				margin: 0;
				font-size: 40px;
			}

			.container {
				max-width: 1000px;
				margin: 0 auto;
				padding: 20px;
				display: flex;
				flex-wrap: wrap;
				justify-content: space-between;
				margin-top: 100px;
				opacity: 0;
				animation: transitionIn 1s forwards;
			}

			@keyframes transitionIn {
				from {
					opacity: 0;
				}
				to {
					opacity: 1;
				}
			}

			video {
				width: 100%;
				height: auto;
			  }
			.fade-out {
				opacity: 0;
				transition: all 0.3s ease-in-out;
			}
			.fade-in {
				opacity: 1;
				transition: all 0.3s ease-in-out;;
			}
			.card {
				background-color: #fff;
				border-radius: 5px;
				box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
				margin-bottom: 20px;
				width: 48%;
				padding: 20px;
				box-sizing: border-box;
			}

			@media screen and (max-width: 700px) {
				.card {
					width: 90%;
				}
			}
			
			.card h2 {
				margin: 0;
				font-size: 28px;
				color: #333;
				margin-bottom: 10px;
			}

			.card p {
				margin: 0;
				font-size: 16px;
				color: #666;
				line-height: 1.5;
			}

			.card a {
				color: #333;
				text-decoration: none;
				font-weight: bold;
				font-size: 16px;
				display: block;
				margin-top: 10px;
			}

			.card a:hover {
				color: #0099ff;
			}
			.hidden {
				display: none;
			}
		</style>
	</head>
	<body class="is-preload">

		<div id="wrapper">
			<header id="header">
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="projects.html">Projects</a></li>
						<!-- <li><a href="blog.html">Blog</a></li> -->
					</ul>
				</nav>
			</header>
			<div id="main">
				<!-- Garviz -->
				<article id="garviz">
					<h2 class="major">GARVIZ</h2>
					<video controls>
						<source src="images\GARVIZ promo.mp4" type="video/mp4"/>
						Your browser does not support the video tag.
						</video>
						<h3 style="margin-top: 100px;">Technologies used:</h3>
						<p style="display: flex; align-items: center; background-color: rgb(212, 210, 210);">
							<img src="images\Python_logo.png" alt="logo1" style="width: 80px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px">
							<img src="images\flask_logo.png" alt="logo2" style="width: 80px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px">
							<img src="images\React_logo.png" alt="logo3" style="width: 80px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px">
							<img src="images\sql_logo.png" alt="logo4" style="width: 80px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px">
							<img src="images\Azure_logo.png" alt="logo5" style="width: 80px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px">
						</p>

					<p>Garviz is an application which helps bringing objectivity in a subjective game like football. It aims to assist clubs, coaches or players who wish to get to the next level by making data driven decision-making. The application has three main functionalitiies.</p>
					<p>GATHER - the place where users can capture all the information for your team. Store data for coaches, players, sessions and matches.</p>
					<p>VISUALIZE - the place where users can look at the story of your team through dashboards. Create reports for team or player performance and share them with relevant stakeholders.</p>
					<p>ANALYZE - the place where users can gain insights that are not possible to catch with the human eye with the help of AI and Machine Learning models.</p>
					<p>The full stack application has been built using React as the front-end framework and Python-Flask as the back-end api which utilizes sql-alchemy ORM to communicate with a MSSQL database. The app has been built with the user in mind, allowing for flexibility and customization. Users can share teams, and assign different priviledges to users within a team. Furthermore, when it comes to gathering event data, the user can define the type of data to me collected. The application takes advantage of JWT authentication and authorization. It has been deployed as a web service using azure cloud services.</p>
					<p>The analyze section will take advantage of data science and machine learning algorithms to uncover hidden patterns in the data that has been gathered from teams. The use of association analysis will help find the most probable outcomes given a user defined scenario while LSTM will allow the users to reflect on playing styles of both teams and players.</p>
					<h3>Link to website:</h3>
					<a href="https://garviz.azurewebsites.net/">https://garviz.azurewebsites.net/</a>
				</article>
				<!-- Predicting patterns of play -->
				<article id="patterns">
					<h2 class="major" style="width: 100%; word-wrap: break-word;">Predicting patterns of play</h2>
					<h3 style="margin-top: 100px;">Technologies used:</h3>
					<p style="display: flex; align-items: center; flex-wrap: wrap; background-color: rgb(212, 210, 210);">
						<img src="images\pandas-logo.png" alt="logo1" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
						<img src="images\numpy-logo.png" alt="logo2" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
						<img src="images\matplotlib-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
					</p>
					<p>A Markov process is a process where predictions can be made regarding future outcomes based on the current state of play. In a given sequence of events, it iterates through each event where for each iteration it pairs the event with its descendant, labelling the first current state and the latter subsequent state. Each state has a list of all the different states (including itself) it can lead to and their frequency, forming a probability of going from it to another one. In the end, random sequences will be generated with an algorithm using those probabilities to form the most probable series of events given a certain event-location is the start.</p>
					<img src="images\patterns-1.PNG" alt="pattern1" style="width: 100%">
					<p>Markov chains are characterized as memoryless meaning that they generate the subsequent event only based on the current one and not on the ones before that. In the context of football, this approach will not provide the full context as it matters whether a player for example receives the ball from a short pass on the ground or from a 40m long ball. More accurate models exists that take those considerations into account and they will be explored in future articles. However, for the purpose of the analysis, the Markov model will be explored in its memoryless form.</p>
					<h3 style="margin-top: 100px;">Methodology:</h3>
					<p>The dataset that will be used is the WSL 2020/2021 season data provided by StatsBomb free data repository. All games that Chelsea WFC have played have been filtered in order to produce a dataset of events which occurred only during their games. Several assumptions have been made to aid the analysis, namely:</p>
					<p>1. The pitch has been split into 30 zones</p>
					<p>Analysing the coordinates of each event independently would not yield any insights due to the variable nature of event coordinates. Hence, the pitch has been split into zone containers each representing a certain range between X and Y coordinates. Figure 1 outlines the zones in which the pitch was split. The zones split the pitch into 6 laterally in order to break down the opposition half into 3rds and into 5 longitudinally in order to include the half-spaces which have become popular in modern football.</p>
					<img src="images\patterns-2.PNG" alt="pattern2" style="width: 100%">
					<p>Events have then been added to their respective containers. This forms a link between the event and the zone it happened which will be represented in the following format:</p>
					<p>{Event Type} — {Event Zone}</p>
					<p>An example itemset of a shot would look like the following:</p>
					<p>(Pass-Zone6) -> (Interception-Zone9) -> (Carry-Zone9) -> (Pass-Zone9) -> (Carry-Zone3)</p>
					<p>2. Carry and Ball Receipt events have been filtered out from the dataset</p>
					<p>Both have been deemed to not add extra value over using just the Pass event in the context of the analysis. Every Pass has a Ball Receipt linked to it which is very rarely positioned in a different zone to the next event, making it redundant. Just like passes are coupled with ball receipts, in the dataset used, every time a ‘Dribble’ event occurs it is coupled with a ‘Carry’ event. Upon inspection, it was determined that ‘Carry’ events provide more information in the context of the analysis and were kept over ‘Dribble’ events.</p>
					<p>3. Possessions against Chelsea have been filtered out</p>
					<p>Since the purpose of this analysis is to concentrate on predicting what Chelsea WFC do when they are in possession, all possessions from other teams have been excluded from the dataset to avoid misleading results.</p>
					<p>Once the dataset has been cleaned, all the event-zone combinations have been added to a list. This list is then iterated through to create a Markov model containing the probabilities of advancing onto any other event-zone combination given the current state. For the purpose of this exercise, this will be sufficient in order to generate stories.</p>
					<p>To generate a random possession, a starting state will need to be passed onto the algorithm as well as the length of the sequence we want to generate. The algorithm will then use the probabilities that have been generated from the Markov model to create a passing sequence to the desired length. In theory, any event-zone combination can be passed as a starting state, but for the purpose of this analysis, only four states will be looked into. Similarly, a possession of any length can be generated and for this analysis, the assumed length of a possession will be five events. Lastly, nine hypothetical possessions will be generated for each state.</p>
					<p>The only step left is to define the states of interest. For this purpose, a heat map, containing all the events during the 20/21 season will be used to determine the most used zones from the team.</p>
					<img src="images\patterns-3.PNG" alt="pattern3" style="width: 100%">
					<p>Figure 2 outlines the commonly used areas by the team while in possession. It can be seen that Chelsea has made use of Zones 11,16 on the left-hand side and Zones 15,20 on the right-hand side. Only zones 16 and 20 will be used as states as they are closer to the opponent’s goal. On top of those 2 zones, passes from Zone 3 will be included in order to represent a probable sequence of events following a goal kick from Chelsea, as well as Zone 21, considered to be one of the zones most frequently leading to a shot as per the previous article.</p>
					<h3 style="margin-top: 100px;">Results:</h3>
					<p>Pass-Zone 3</p>
					<img src="images\patterns-4.PNG" alt="pattern4" style="width: 100%">
					<p>Looking at the nine hypothetical cases that have been generated from the algorithm for ‘Pass-Zone3’ it can be seen that there is an equal spread between resolving to play out from the back and making the decision to play over the first line of the opposition directly to the middle 3rd. An interesting trend can be observed when the team do decide to play over the first line of defence, they mainly resolve to playing in Zone 11 which is on the left-hand side of the pitch. Looking further into the sequence, once the ball reaches Zone 11 it is being progressed forward through the wide areas into Zones 16 and 21 with only one occasion where it is being passed back and recycled(7). When a decision is made to play out from the back, rather unsurprisingly the following four plays do not progress the ball further onto the pitch but is played between players in the zones in front of the goal, possibly to force a reaction from the opposition.</p>
					<p>Pass-Zone 16</p>
					<img src="images\patterns-5.PNG" alt="pattern5" style="width: 100%">
					<p>The generated sequences starting with Pass-Zone16 show a preference for slow build-up instead of a direct approach. Upon inspection, only one sequence (2) presents an attack which ends up in Zone 28. Instead, the algorithm insists that it is more likely for the team to make short passes in the attempt to consolidate possession or to move the ball to the other side of the pitch. There has also been a lack of long ball switches from the left-hand side to the right-hand side which serves as a further indication of the team’s preference to play short passes when in that area.</p>
					<p>Pass-Zone 20</p>
					<img src="images\patterns-6.PNG" alt="pattern6" style="width: 100%">
					<p>Similar to the previous case, Pass-Zone20 offers little difference in principle to what was observed when the team was in a similar position but on the other side of the pitch. The most probable sequence in this area consists of short passes which aim to consolidate possession or to switch the area of play to the other side of the pitch. Two outliers are presented in (3) and (5) which show attempts for a more direct play.</p>
					<p>Pass-Zone 21</p>
					<img src="images\patterns-7.PNG" alt="pattern7" style="width: 100%">
					<p>Looking at Pass-Zone21 becomes more interesting as the area is positioned in the attacking 3rd of the pitch and one would expect to see more assertive plays to attack the opposition goal. And indeed a shot has been included in the sequence in three (1),(3), and (8) of the nine generated samples which can serve as an indication of the team’s potency to create a shooting chance from that area. In those cases, the shot has been preceded by either a cross (1),(8) or a switch into the opposite half-space, something that could be worth analysing in depth. It can also be observed the high presence of Pressure events which can serve as an initial indication of counter-pressing tactical instructions in the final 3rd.</p>
					<h3 style="margin-top: 100px;">Conclusion:</h3>
					<p>In this article, a season-long dataset of events for Chelsea WFC has been analysed and a Markov chain has been constructed to calculate the probabilities of transitioning from one state to another. This has then been used to simulate possessions with a pre-determined starting point. For the purpose of the analysis 4 starting points have been analysed namely: a goal kick Pass-Zone3 passes starting in the most active areas for the team Pass-Zone16 and Pass-Zone20 and passes from an area determined to be potent for creating shots Pass-Zone21. By running only nine simulations, some potentially interesting insights can be observed like the team preferring to pass to the left-hand side of the pitch if it makes a decision to skip the first line of defence or that when in Zone20 or Zone16 the team would rather consolidate possession than look for a direct approach. Pass-Zone21 had 30% of its simulations ending with a shot which confirms its potency to create shots, it also visually recreated the way that is most likely for the team to create a shooting chance — with crosses.</p>
					<p>Markov chains are stochastic in nature and only nine simulations are not a big enough sample size to reach any conclusions. Having a higher number of simulations would generate a set of sequences which would together contain a more accurate representation of what a given team is capable of. Those can then be labelled and fed into the coaching staff providing the insights that could allow for a better understanding of the analysed team.</p>
					<p>In this case, five events in total have provided satisfactory results but they can be changed to reflect the needs of the user.The accuracy of the probabilities can be further increased by taking into account the action that led to the current state, however for that to be feasible, a bigger dataset is required.</p>
					<h3>Useful links:</h3>
					<a href="https://github.com/PWolf96/Football-Data-Science">Github: Predicting patterns of play</a><br>
					<a href="https://medium.com/@pv1g15/predicting-patterns-of-play-e0817d0d7055">Medium: Predicting patterns of play</a><br>
				</article>
				<!-- Player contribution - 1 -->
				<article id="contribution1">
					<h2 class="major" style="width: 100%; word-wrap: break-word;">Player contribution</h2>
					<h3 style="margin-top: 100px;">Technologies used:</h3>
					<p style="display: flex; align-items: center; flex-wrap: wrap; background-color: rgb(212, 210, 210);">
						<img src="images\pandas-logo.png" alt="logo1" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
						<img src="images\numpy-logo.png" alt="logo2" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
						<img src="images\matplotlib-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
					</p>
					<p>This article will explore the possibility of evaluating player contribution to shots. Inspired by an article written by StatsBomb, a first-order Markov model will be constructed to take into account all events that have happened in a single season for Chelsea WFC and evaluate their contribution towards a shot.</p>
					<h3 style="margin-top: 100px;">First order Markov chain</h3>
					<p>A Markov chain is a random process describing a sequence of possible events in which the probability of each event depends on events that happened before. This analysis will use a first-order Markov chain meaning that the probability of each event depends only on the state attained in the previous event. A graphical representation of the chain is given in Figure 1. The chain is divided into transition states and absorption states.</p>
					<img src="images\markov1-1.PNG" alt="markov1-1" style="width: 100%">
					<p>States that have a probability of transitioning into another state are called transient states.</p>
					<p>States in which the probability of remaining in the state is 1 are called absorption states.</p>
					<p>Once the probabilities involving all states are calculated, a matrix is constructed to reflect those probabilities called a transition matrix. Similarly, an absorption matrix is constructed reflecting the probabilities of each transient state resulting in an absorption state. The probability of reaching an absorption state from a given transient state in n-steps is calculated. A fundamental matrix is derived from the transition matrix which is then used to calculate the expected number of plays as its row sums. Finally, the ‘contribution factor’ is calculated by the product of the fundamental matrix and the absorption matrix.</p>
					<h3 style="margin-top: 100px;">Methodology:</h3>
					<p>To achieve the analysis, the necessary cleaning process needs to be carried out. As with previous analyses, the cleaning starts with splitting the pitch into 30 zones and excluding the events which are determined to not add any additional value to the possession chains. Furthermore, only shots which meet a certain xG threshold would be considered shots. For a more detailed explanation of these steps, previous posts can be checked.</p>
					<img src="images\patterns-2.PNG" alt="markov1-2" style="width: 100%">
					<p>For absorption states two possible outcomes will be considered, namely:</p>
					<p>- Shot</p>
					<p>- Loss of possession</p>
					<p>Whether the possession ends with a shot, which is considered a positive outcome, or loss of possession, a negative outcome. A decision has been made to include shots as an outcome instead of goals as even though goals provide the highest value in the context of a football game, some valuable plays which do not end up in a goal get missed out. Similarly with shots, there will be valuable plays which will not result in a shot. However, less valuable plays will be missed out through tracking shots, than goals since the first one occurs more frequently than the latter. In that respect, it is believed that having shots as an outcome will capture the true value of a player to a greater extent.</p>
					<p>In terms of transition states, for the purpose of this analysis, we are interested in the zones which contribute to a positive outcome the most. For this reason, each zone will be considered a transient state. On top of that four more set-piece scenarios have been added, namely:</p>
					<p>- Goal kick</p>
					<p>- Throw in</p>
					<p>- Corner</p>
					<p>- Free kick</p>
					<p>Combined together there are 34 transient states which will be used.</p>
					<p>Once the transient and absorption states are known, the dataset would need to be manipulated to accommodate for the analysis. The dataset is divided into possession sequences where each possession sequence is considered individually. Each event in the possession is then coupled with its descendant, meaning that for any given event, the current state is the zone or set-piece that it has occurred in and the next state is the zone or set-piece of the next event in the couple. An exception is only made for the last event of a possession sequence where the next state ends in either a ‘shot’ or a ‘loss of possession’. Events after manipulation are shown in Table 1.</p>
					<img src="images\markov1-2.PNG" alt="markov1-3" style="width: 100%">
					<p>A Markov model is then run on the dataset to generate the probabilities of going between transient states and absorption states. This will then be used to calculate the shot contribution value of each transient state. This will be achieved with the following matrices:</p>
					<p>(1) 34x34 transition matrix — containing the probabilities of each transition state to end up in any other transition state</p>
					<p>(2) 34x2 absorption matrix — containing the probabilities of each transition state to end up in the respective absorption state</p>
					<p>(3) 34x34 identity matrix</p>
					<p>(4) 34x34 fundamental matrix — the inverse of the identity matrix minus the transition matrix</p>
					<p>The probability of each transition state leading to an absorption state is calculated by the product of the fundamental matrix and absorption matrix. When the probabilities are known, the player contribution can be calculated by assessing if the play done by the particular player increases the probability of a shot or decreases it. When the next event is a shot the probability is equal to 1 and if it is a loss in possession then it is equal to 0.</p>
					<p>Player contribution = Probability of a shot | Next event — Probability of a shot | Current event</p>
					<h3 style="margin-top: 100px;">Results:</h3>
					<p>Results of the probabilities have been presented in Figures 2 and 3.</p>
					<img src="images\markov1-3.PNG" alt="markov1-4" style="width: 100%">
					<img src="images\markov1-4.PNG" alt="markov1-5" style="width: 100%">
					<p>Figure 2 shows that rather unsurprisingly, zone 28 is the area which has the highest probability of preceding a shot. It is followed by corners which serves as an indication that the team takes advantage of corner set-pieces to generate shots. It can be observed that the left-hand side area has a slight edge for shot generation Zone26 narrowly edges Zone 24 and is followed by Zone 22 whilst Zone29 and Zone27 have a fairly similar probability. On the other end of the field, as expected, are the areas which contribute to a shot the least, with the lowest being Zone1 and Zone5.</p>
					<p>The derived equation for player contribution has been used to evaluate each event. In the end, all events in which a particular player has been involved have been summed to calculate the player’s season contribution towards shots for the team. The results are presented in Figure 4.</p>
					<img src="images\markov1-5.PNG" alt="markov1-6" style="width: 100%">
					<p>Analysing Figure 4 provides some interesting insights. Unsurprisingly, the model determines Sam Kerr as the player with the highest contribution towards shots with Pernille Harder and Francesca Kirby second and third. This could be largely influenced by those three players being the ones with the most shots on the team, meaning that they convert chances into shots which would eventually score goals. On the other hand, it could also serve as an indication that those players progress the ball successfully into more dangerous areas more often than not. On the other side, we can see that Bethany England has the lowest shot contribution from the team. The result can be broken down into a few potential reasons. Firstly, the model punishes giving possession away, meaning that if the play style of the player is to attempt many aggressive and high-risk passes albeit most of them being unsuccessful, it would drive down contribution due to the fact that the ball is given away more times than chances have been created. Secondly, players which tend to be on the other side of the spectrum and play too many safe passes in advanced areas are also punished by the model since instead of progressing the ball into an area with a higher possibility of a positive outcome, the ball is recycled and chances are not taken. Figure 5 represents the difference between the current model and the one that was built, taking into account if the player was under pressure during the event. In some cases, a player not being under pressure in deeper zones might prove to be more impactful for creating a shot than being under pressure in an advanced area. With that regard, the model which includes pressure has the potential to be more accurate.</p>
					<img src="images\markov1-6.PNG" alt="markov1-7" style="width: 100%">
					<p>It can be observed that the values have not changed much, mainly due to the fact that in the most dangerous areas, players will very rarely not be under pressure.</p>
					<p>It is important to clarify that the model does not take into account any tactical considerations where players are tasked with the responsibility of consolidating play or ones who are given the freedom to attempt riskier passes with the possibility of making that one killer pass that will ultimately decide a game. Different models can then be constructed, using different transition and absorption states to capture any specific coaching requirements. Furthermore, the model does not take into account the context of the event. Details such as state of the game, opponent or occasion have not been taken into account. Differentiating between whether the player was under pressure or not did not provide significant changes. Estimation accuracy can also be improved by implementing a higher-order Markov model which takes into account the previous state before reaching the current one. Lastly, the model does not consider how events transition from one state to another. A Markov decision process could be constructed to accommodate that need.</p>
					<h3 style="margin-top: 100px;">Conclusion:</h3>
					<p>A first-order Markov model has been constructed and used to analyse the 20/21 season of Chelsea WFC. It aimed to evaluate players based on their contribution towards creating shooting chances and taking shooting opportunities. It was also used to get a visual representation of which zones have the highest probability of creating a shooting chance where it was concluded that the left-hand side is slightly more dangerous than the right-hand side with corners possessing the second highest threat of a shot after Zone 28. Players playing in forward or midfield positions yield the highest shot contribution. Defenders having low shot contribution values can serve as an indication that Chelsea does not use defenders as much to progress the ball into dangerous positions. This can be a result of tactical instructions or a lack of qualities to do so. Interesting insights have been extracted when looking at the least contributing players, which can lead to analysis in further detail when raised to the coaching staff.</p>
					<p>Player evaluation has always been a crucial part of any competitive team. Recognising players as ones who perform up to standard and on the other hand, players who need to be developed further is embedded into the work cycle of any coaching staff. Before the digital era, conclusions have been deducted primarily based on subjectivity. However, technology has given the opportunity for a more objective approach to player evaluation based on pre-agreed criteria. A vast amount of parameters can be taken into account over a long time span, helping to uncover certain players’ merit, which could have been invisible to the naked eye. In doing so, a team can not only recognise and retain talent more effectively but also scout and uncover hidden gems which can vastly improve certain aspects of the team, something which in the long run can make a difference.</p>		
					<h3>Useful links:</h3>
					<a href="https://github.com/PWolf96/Football-Data-Science">Github: Evaluating player contribution</a><br>
					<a href="https://medium.com/@pv1g15/evaluating-player-contribution-f9faea52238b">Medium: Evaluating player contribution</a><br>
				</article>
				<!-- Player contribution - 2 -->
				<article id="contribution2">
					<h2 class="major" style="width: 100%; word-wrap: break-word;">Player contribution</h2>
					<h3 style="margin-top: 100px;">Technologies used:</h3>
					<p style="display: flex; align-items: center; flex-wrap: wrap; background-color: rgb(212, 210, 210);">
						<img src="images\pandas-logo.png" alt="logo1" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
						<img src="images\numpy-logo.png" alt="logo2" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
						<img src="images\matplotlib-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-bottom: 20px; margin-top: 20px" >
					</p>
					<h3 style="margin-top: 100px;">Methodology:</h3>
					<p>The methodology is the same as in the first-order Markov chain analysis with a few notable exceptions.</p>
					<p>Zones 1 to Zone 15 have been condensed to Zone 1 — Zone 3 as outlined in Figure 1 below. Reason being that implementing a second-order model contributes to a significant increase in transient states which would be computationally demanding.</p>
					<img src="images\markov2-1.PNG" alt="markov2-1" style="width: 100%">
					<p>The two absorption states have been kept as Loss of possession and Shot. However, the transient state corresponds to the pairing of the current state with its preceding state. Having 18 zones combined with 4 set-piece states amounts to a total of 484 transient states to be considered. Table 1 presents possession 5, manipulated to suit a second-order model.</p>
					<img src="images\markov2-2.PNG" alt="markov2-2" style="width: 100%">
					<p>Constructing the matrices proved to be not as straightforward as in the first-order model. Having a second-order model firstly increased the amount of possible scenarios more than tenfold. Secondly, it resulted in some highly unlikely combinations for example:</p>
					<p>Zone 1 — Zone 30 -> Zone 30 — Zone 1</p>
					<p>Zone 1 — Corner -> Corner — Zone 3 etc.</p>
					<p>This would result in many zero-sum columns in the matrices which is problematic. Excluding those whilst maintaining a square matrix would result in data loss. However, in the context of a football game, it is assumed that any events that have been linked to any of the pruned scenarios would have not had any significant contribution to shot generation. Therefore a decision has been made to prune the zero-sum cases which would reduce the transient states to 394.</p>
					<p>Finally, the player contribution is calculated using the following formula:</p>
					<p>Player contribution = Probability of a shot | Next event pair — Probability of a shot | Current event pair</p>
					<h3 style="margin-top: 100px;">Results:</h3>
					<p>Figure 3 presents the results from the second-order model compared to the first-order one.</p>
					<img src="images\markov2-3.PNG" alt="markov2-3" style="width: 100%">
					<p>Looking at Figure 3 it can be concluded that applying a second-order Markov model does make a difference in the outcome for shot contribution. Most of the players have had a positive change in contribution with a notable exception being Sam Kerr. This implies that most players have been losing possession when there was little possibility to progress the attack, meaning that when they get an opportunity to create a shot they are converting more than the first-order model suggests. Another reason can be the fact that the zones have been reduced to 18 which lowers the shot possibility of zones closer to the halfway line as they are put in the same category as the least shot potent zones. Although absolute contribution has improved, when comparing players, the order has not changed dramatically, meaning that all players benefit from the second-order model in the same manner. Notable exceptions to that are Ann-Katrin Berger and Millie Bright who seem to have benefited the most from the model.</p>
					<h3 style="margin-top: 100px;">Conclusion:</h3>
					<p>Implementing a second-order Markov model suggests that including the preceding event in the context of the current event makes a difference in the probability of creating a shooting opportunity. That is to the surprise of any person who has watched a game of football. However, it was interesting to see how certain players have benefited more than others, suggesting a certain play style. Insights can be derived from comparing the two models alone since it has proven that it highlights players who are frequently on the receiving end of an unfavourable event, whether it is a pass, a clearance or a dribble. Analysing all 30 zones would provide an even more accurate estimate of the above, however, such a process is computationally demanding and could not be performed.</p>
					<p>Football sometimes is a cruel game. Watching a certain player who has not had an impact may suggest that he has had little contribution and most conventional statistics can back that claim, where in reality there was little opportunity to contribute in the first place. Models like this highlight such cases. The context can then be further examined and tactical considerations can be put in place in order to improve performance. Something which in the long run can make a difference.</p>
					<h3>Useful links:</h3>
					<a href="https://github.com/PWolf96/Football-Data-Science">Github: Evaluating player contribution - 2nd order</a><br>
					<a href="https://medium.com/@pv1g15/evaluating-player-contribution-continued-1f5855636dcd">Medium: Evaluating player contribution - 2nd order</a><br>	
				</article>
				<!-- Predicting playstyles -->
				<article id="playstyles">
					<h2 class="major" style="width: 100%; word-wrap: break-word;">Predicting playstyles</h2>
					<h3 style="margin-top: 100px;">Technologies used:</h3>
					<p style="display: flex; align-items: center; max-width: 100%;  flex-wrap: wrap; background-color: rgb(212, 210, 210);">
						<img src="images\pandas-logo.png" alt="logo1" style="width: 150px; padding: 0 20px;" >
						<img src="images\numpy-logo.png" alt="logo2" style="width: 150px; padding: 0 20px;" >
						<img src="images\matplotlib-logo.png" alt="logo3" style="width: 150px; padding: 0 20px;" >
						<img src="images\nltk-logo.png" alt="logo3" style="width: 100px; padding: 0 20px;" >
						<img src="images\pytorch-logo.png" alt="logo3" style="width: 180px; padding: 0 20px;" >
						<img src="images\scikit-learn-logo.png" alt="logo3" style="width: 120px; padding: 0 20px; margin-bottom: 20px" >
						<img src="images\seaborn-logo.png" alt="logo3" style="width: 150px; padding: 0 20px;" >
					</p>
					<p>This analysis will attempt to use LSTM models to classify the playing styles of teams based on possession sequences. Three main categories will be defined which represent the main styles of play that a team can produce namely Slow build-up, Direct play and Control based play. Two more categories are added in Set-pieces and Uncategorized to allow for isolated events and less organized sequences to be classified.</p>
					<h3 style="margin-top: 100px;">LSTM Neural Networks</h3>
					<p>Long short-term memory is a type of recurrent neural network that can process not only single data points but also entire sequences of data. The name is derived from having the ability to store and take into consideration information not only from short-term memory but also retained information from previous states called ‘long-term memory‘.</p>
					<p>LSTM models have 3 gates and 1 state:</p>
					<p>The cell state is used to store the ‘long-term memory’. It starts from the beginning of a cycle, gets updated if necessary, and then is used in conjunction with the input to produce an output.</p>
					<p>The forget gate is used to update the cell state in the beginning. It takes into consideration the input and the ‘short-term memory’ from the previous event and updates the cell state accordingly.</p>
					<p>The input gate is used to add information from the new event to the previous knowledge in the cell state.</p>
					<p>The output gate takes into consideration information fed from the cell state combined with the previous output data to return a new output.</p>
					<img src="images\playstyles1.PNG" alt="playstyles1" style="width: 100%">
					<h3 style="margin-top: 100px;">Methodology:</h3>
					<p>1. Labelling sequences</p>
					<p>The labels are one of the most important decisions that need to be made in a classification exercise. In football, teams can have</p>
					<p>Slow build-up — A sequence containing more than 5 short-medium length passes which aim to advance the ball into dangerous areas.</p>
					<img src="images\playstyles2.PNG" alt="playstyles2" style="width: 100%">
					<p>Set-Piece — Free-kicks, Corners, Throw-ins</p>
					<img src="images\playstyles3.PNG" alt="playstyles3" style="width: 100%">
					<p>Direct — A sequence that contains mainly forward passes which aim to progress the ball into dangerous areas as quickly as possible.</p>
					<img src="images\playstyles4.PNG" alt="playstyles4" style="width: 100%">
					<p>Control — A sequence containing many forward and backward passes which aim to keep the current state of the game.</p>
					<img src="images\playstyles5.PNG" alt="playstyles5" style="width: 100%">
					<p>Uncategorized — Sequences that do not fall under any of the other four categories.</p>
					<img src="images\playstyles6.PNG" alt="playstyles6" style="width: 100%">
					<p>2119 sequences in total from season 20–21 season have been labelled</p>
					<p>1. Prune event information</p>
					<p>Each event in the database contains 173 pieces of information which would be computationally demanding if used without any adjustments. For this reason, a focus has been given to information that is critical to identifying the playstyle, namely:</p>
					<p>Type of event, play pattern, player position, pressure, location of events and various crucial details about specific event types like pass length, pass height etc. After pruning was performed, the information for each event was reduced to 33 pieces of information.</p>
					<p>2. One-hot encoding all categorical information</p>
					<p>As big part of the data comes in categorical textual format, encoding needs to be performed. Most of the textual data is in a nominal form and is, therefore, one-hot encoded. One-hot encoding avoids the hierarchical assumptions of other type of encodings which are mainly targeting ordinal data. There were also Boolean values which would be mapped as 1 if true and 0 if false.</p>
					<p>3. A threshold of 60 events per sequence has been established</p>
					<p>Out of all 2119 sequences, there were only 8 sequences above 60 with the highest being 79 events. In order to increase efficiency, those 8 sequences have been excluded from the analysis.</p>
					<p>4. Balancing labels</p>
					<p>The initial labelling of playing style indicates that there is an imbalance in the label occurrence as outlined in figure 3.</p>
					<img src="images\playstyles7.PNG" alt="playstyles7" style="width: 100%">
					<p>Slow build-up is almost 8 times more frequent than Control. An oversampling has been applied by resizing each label to achieve a more balanced distribution.</p>
					<p>After being balanced, the dataset was split into training and testing with an 80/20 split and a dictionary was created from the tensors of the sequence and its respective label. Finally, an LSTM model has been applied using PyTorch and PyTorch with cross-entropy has been used as the loss function.</p>
					<h3 style="margin-top: 100px;">Results:</h3>
					<p>Results from the LSTM model are presented in Figures 2 and 3. A high validation accuracy has been achieved with 91%. However, it can be seen that validation loss is higher than training loss, with the minimum loss achieved is 39%. It remains stable at around 40–45% which is a positive sign that the model does not overfit. The relationship between accuracy and loss indicates that although the absolute prediction is accurate most of the time, the model is not confident in it. That kind of behaviour is to be expected due to the high variability of the data inputs and the lack of a strong definition of each label.</p>
					<p>Different scenarios have been run as shown in figures 2 and 3 trying to improve the validation loss. However, it can be seen that results remain pretty stable across all four different scenarios.</p>
					<img src="images\playstyles8.PNG" alt="playstyles8" style="width: 100%">
					<img src="images\playstyles9.PNG" alt="playstyles9" style="width: 100%">
					<p>The achieved validation loss will be deemed satisfactory due to the nature of the analysis. As discussed above football playing patterns do not have a specific definition. They are open to interpretation and are heavily dependent on context. Even though an attempt has been made to define a set of rules to aid labelling, the highly varying nature of the sequences decreases the confidence in the predictions. Moreover, in this analysis, only the critical information to identify the event location and type has been included. Working with a bigger sample size would allow for more context-defining variables to be included which should, in theory, help the model in its predictive confidence. Results shown from the confusion matrix in Figure 4 show that slow build-up is the one with the lowest accuracy and is mostly mistaken for direct or control play styles. This is to be expected due to the fact that both playing styles can be quite similar to each other when taking into account only event coordinates and event count. However, even without including context, the model was able to predict correctly almost 80% of the time. Uncategorized data was mostly mistaken for direct plays and set-pieces. This could be because some of the labelled direct plays and set-pieces contain only one event which is most of what the uncategorized labels are made of. It can be observed that direct plays are not mistaken for control plays, mainly due to the fact that by definition, one play style should presumably contain much shorter sequences than the other, something which the model has recognized. Lastly, it can be seen that control plays have been predicted 100% of the time, which could be largely due to oversampling that was done earlier.</p>
					<img src="images\playstyles10.PNG" alt="playstyles10" style="width: 100%">
					<p>The predictive power of the model can be enhanced by including information on the context of a particular sequence. Information like the types of passes (differentiating between risky and safe) or pressure applied could significantly improve the model. Furthermore, more specific definitions for labelling could also increase the confidence in which the model predicts.</p>
					<h3 style="margin-top: 100px;">Conclusion:</h3>
					<p>The higher the level that football is played, the smaller the differentiations that separate the winners from the losers. The more difficult it becomes for the naked eye to spot and pinpoint an objective reason for a certain outcome. Moreover, no human can catch all details in a game and even experienced professionals would require highly concentrated efforts before labelling certain playing sequences and reaching the stage where they ask the right questions. Having a tool which significantly shortens the amount of time needed to ask the right questions would allow analysts to spend more time finding the answers to those questions.</p>
					<p>An LSTM model was used to classify possession sequences into playing styles. The achieved accuracy was 91% on the best iteration and the loss was 40% which, considering the complex nature of the sequences, was deemed to be satisfactory.</p>
					<p>Such models outline initial steps towards being able to objectively classify complex datasets such as the ones of a free-flowing game of football. The same approach can be used to classify specific events which teams would like to bring attention to during opposition or team analysis. It could also be used towards understanding the habits of players and putting them into a category which can then be used to assess if one is a good fit for a certain system.</p>
					<p>Attention is brought to many events or sequences that share similar contexts and are classified as being the same in an efficient manner, without needing to spend a vast amount of time. The saved time can then be utilized to break them down to the finest details and then find patterns to be able to come close to the way people involved in the decisions think and more importantly maybe uncover more pieces of the puzzle towards a certain outcome. Something which in the long run can make a difference.</p>
					<h3>Useful links:</h3>
					<a href="https://github.com/PWolf96/Football-Data-Science">Github: Predicting playstyles</a><br>
					<a href="https://medium.com/@pv1g15/classifying-playstyles-cb5b3126a9be">Medium: Predicting playstyles</a><br>
				
				</article>

				<article id="stock-price">
					<h2 class="major" style="width: 100%; word-wrap: break-word;">Predicting stock prices</h2>
					<h3 style="margin-top: 100px;">Technologies used:</h3>
					<p style="display: flex; align-items: center; max-width: 100%;  flex-wrap: wrap;background-color: rgb(212, 210, 210);margin-top: 20px; margin-bottom: 20px">
						<img src="images\pandas-logo.png" alt="logo1" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\numpy-logo.png" alt="logo2" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\matplotlib-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\scikit-learn-logo.png" alt="logo3" style="width: 120px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\seaborn-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-top: 20px;margin-bottom: 20px" >
						<img src="images\keras-logo.png" alt="logo3" style="width: 80px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >	
					</p>
					<h3 style="margin-top: 100px;">Project overview</h3>
					<p>This project will explore the possibility of analyzing time-series data in the form of stock prices in order to predict future growth or decline. For this purpose, a Long-Short Term Memory (LSTM) methodology will be adopted. The methodology will use a user-defined set of consecutive days that will train a model to predict the following days’ closing price of Google. In order to do that, the model will be trained on readily available data from Yahoo Finance. In conjunction with that, additional features will be added to increase the model accuracy. Finally, different timeframes of lookbacks will be compared to each other, in the attempt to find the most optimal solution.</p>
					<h3 style="margin-top: 100px;">Problem statement</h3>
					<p>The task is to create an LSTM model that will predict the next day closing price as accurately as possible, based on pre-defined variables and in a user-defined look-back time frame. This will be achieved by doing the following:</p>
					<p>1. Using Yahoo Finance API to extract data for Google stock prices</p>
					<p>2. Pre-process the dataset, including additional features and splitting the dataset into sets of a length defined by the user, containing a tensor for the chosen variables and a matrix for the prices that match the variables</p>
					<p>3. Build an LSTM model</p>
					<p>4. Train the model on existing data</p>
					<p>5. Predict the later part of the dataset by using the model</p>
					<p>6. Compare the predicted values with the actual values</p>
					<p>7. Tune model parameters and user inputs to improve results</p>
					<h3 style="margin-top: 100px;">Metrics</h3>
					<p>Mean Square Error — this metric measures the amount of error in the statistical model. Although it is not a determining metric for the success of the model due to the nature of the problem, keeping the MSE as low as possible gives an indication of a good model.</p>
					<p>Trend — The more critical metric for the model evaluation will be the trend. A model which follows the trend of the actual price is able to predict growth or decline, which in the nature of the problem is considered to be satisfactory.</p>
					<h3 style="margin-top: 100px;">Data exploration</h3>
					<p>Stock data for Google has been taken from Yahoo Finance for the period of 01/10/2004–01/10/2022.</p>
					<code>
						def get_stock_data(symbol, start_date, end_date):<br>
							stockData = web.DataReader(symbol, 'yahoo', start_date, end_date)<br>
							return stockData<br>
							<br>
						start_date = dt.datetime(2004, 10, 1)<br>
						end_date = dt.datetime(2022, 10, 1)<br>
						df_GOOG = get_stock_data('GOOG',start_date, end_date)
					</code>
					<p>A description of the values contained in the dataset have been shown in Table 1.</p>
					<img src="images\stock-price-1.PNG" alt="stock-price1" style="width: 100%">
					<p>Looking at the count, it can be seen that there are no missing values. Furthermore, we can see the sharp difference between the mean, 50% and 75% and the maximum values for high, low and open. Giving an early indication of the potential spike in stock price to be observed.</p>
					<p>Figure 1 shows the correlation between the different variables in the database. It can be seen that every variable apart from the Volume is perfectly correlated to one another.</p>
					<img src="images\stock-price-2.PNG" alt="stock-price2" style="width: 100%">
					<img src="images\stock-price-3.PNG" alt="stock-price3" style="width: 100%">
					<p>In figure 2, the stock price has been plotted with respect to time. It can be observed that for the beginning of the analyzed period, the price of the stock has been steadily increasing. However, towards the end, volatility can be observed, where the price is going through periods of sharp declines and sharp upward trends.</p>
					<h3 style="margin-top: 100px;">Algorithms and Techniques</h3>
					<p>Long short-term memory is a type of recurrent neural network that can process not only single data points but also entire sequences of data. The name is derived from having the ability to store and take into consideration information not only from short-term memory but also retained information from previous states called ‘long-term memory‘.</p>
					<p>LSTM models have 3 gates and 1 state:</p>
					<p>The cell state is used to store the ‘long-term memory’. It starts from the beginning of a cycle, gets updated if necessary, and then is used in conjunction with the input to produce an output.</p>
					<p>The forget gate is used to update the cell state in the beginning. It takes into consideration the input and the ‘short-term memory’ from the previous event and updates the cell state accordingly.</p>
					<p>The input gate is used to add information from the new event to the previous knowledge in the cell state.</p>
					<p>The output gate takes into consideration information fed from the cell state combined with the previous output data to return a new output.</p>
					<img src="images\stock-price-4.PNG" alt="stock-price4" style="width: 100%">
					<h3 style="margin-top: 100px;">Methodology</h3>
					<p>Importing relevant libraries</p>
					<p>Figure 4 shows the libraries that have been imported in order to proceed with the analysis. Libraries include common additions like pandas, numpy, seaborn, sklearn and keras. One library to note is the finta library used to calculate the technical indicators which will be added to the dataset, to be discussed later.</p>
					<code>
						import datetime as dt<br>
						from finta import TA<br>
						<br>
						import pandas as pd<br>
						from pandas_datareader import data as web<br>
						import numpy as np<br>
						<br>
						import matplotlib.pyplot as plt<br>
						import seaborn as sns<br>
						<br>
						from sklearn import preprocessing<br>
						from sklearn.metrics import mean_squared_error<br>
						from numpy import size<br>
						from sklearn.model_selection import train_test_split<br>
						from sklearn.preprocessing import MinMaxScaler<br>
						<br>
						import tensorflow as tf<br>
						<br>
						import keras<br>
						from keras.models import Sequential<br>
						from keras.models import Model<br>
						from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate<br>
						from keras import optimizers<br>
						from keras.callbacks import History<br>
					</code>
					<p>Data Pre-processing</p>
					<p>After loading the dataset, there are some necessary steps which need to be performed in order to ensure firstly, the accuracy of the prediction is as high as possible and secondly, making sure that parameters are passed into the LSTM model in their correct form.</p>
					<p>After the initial exploration of the dataset it has been determined that additional features will be required in order to increase the accuracy of the prediction. The features are in the form of technical indicators used to measure the performance of the stock in various ways. Technical indicators are split into different types, namely: trend , momentum, volatility and volume. Even though it might be tempting to include as many of those indicators as possible, adding such requires caution. If many indicators of the same type are added, the model will suffer from multicollinearity, overblowing the results in one direction or another. For this reason, it is advised that a mixture of technical indicator types are used in a way that will complement each other. Different types of strategies exist when one is looking to interpret the stock market in an attempt to make predictions on the future price of a stock. One such includes using Relative Strength Index together with Bollinger Bands. Attention needs to be placed on observing if the prices hit any of the upper or lower bands whilst looking after the RSI value to assess if the stock is overbought or oversold. Such indicators can server as an early signal for investors for a reversal in the trend of a stock.</p>
					<p>For the purposes of this analysis the following technical indicators have been chosen:</p>
					<p>- Relative Strength Index (RSI) — Used as a momentum indicator, commonly used to determine the strength of price changes</p>
					<p>- On Balance Volume — Used as a volume indicator, calculating the buying and selling pressures of a stock</p>
					<p>- Bollinger Bands — Used as a volatility and trend indicator, plots three trend lines, determining whether a stock is overbought or oversold</p>
					<p>Once all the technical indicators are included, the dataset would need to be split into a dataset used to train the model and one that will be used to test the model on and compare its predictive power. The split will follow the standard practices by allowing for 70% of the dataset for training and 30% for testing.</p>
					<p>Once the split has occurred, the datasets need to be normalized in order to facilitate the different ranges of values such as volume and price. It is also important to normalize the train and test datasets separately in order to prevent data leakage from one dataset to another. Sklearns’ function MinMaxScaler will be used to normalize the data of the train set. Then that scaler will be used to transform the variables of the testing dataset.</p>
					<code>
						#Split data<br>
						train_data = df[:split_index]<br>
						test_data = df[split_index:]<br>
						<br>
						#Normalize the variables<br>
						normaliser = preprocessing.MinMaxScaler(feature_range=(0,1))<br>
						#Creating a specific scaler for the y values<br>
						y_normaliser = preprocessing.MinMaxScaler(feature_range=(0,1))<br>
						#Normalizing the training dataset by using fit_transform<br>
						train_normalised = normaliser.fit_transform(train_data)<br>
					</code>
					<p>Once splitting and normalizing is accounted for, the training and testing datasets need to be further split into 2 arguments. One includes the list of preceding events before a price prediction and the other one includes the price prediction. The above is done via a method to convert the dataset into a tensor containing all the lists of preceding events, the length of which is defined by the user, where each entry in the list keeps all of its technical indicators. Then from the passed dataset, only the next day prices are collected following the list of events, ensuring a 1-day lag. In the end, a combination of a next day price combined with its n preceding events is achieved.</p>
					<code>
						# split a multivariate sequence into samples<br>
						def split_sequences(sequences, n_steps):<br>
							'''<br>
							A method for preprocessing arrays for LSTM modelling. The sequence will be split into two:<br>
							- A tensor containing groups of all the variables of a user-defined set of time-steps before the price to be predicted<br>
							- A matrix containing the prices to be predicted<br>
							<br>
							Args:<br>
							-sequences - the sequence to be pre-processed<br>
							-n_steps - user-defined time-steps before the predicted price<br>
							<br>
							Output:<br>
							X - the tensor<br>
							y - the matrix <br>
							'''<br>
							<br>
							X, y = list(), list()<br>
							for i in range(len(sequences)):<br>
								# find the end of this pattern<br>
								end_ix = i + n_steps<br>
								# check if we are beyond the dataset<br>
								if end_ix > len(sequences):<br>
									break<br>
								# gather input and output parts of the pattern<br>
								seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]<br>
								X.append(seq_x)<br>
								y.append(seq_y)<br>
							return np.array(X), np.array(y)<br>
					</code>
					<p>Model</p>
					<p>The model that has been built includes 4 LSTM layers, each followed by a Dropout. All 4 LSTM layers have been modelled to read the input data and output 70 features and have a relu activation function. The optimizer used for the model is adam. The model is also trained for 25 epochs with a batch size of 32.</p>
					<code>
						def build_lstm(X_train, y_train):<br>    
						'''<br> 
						A method to train an LSTM model using 4 LSTM layers, 1 Dense layer and a relu activation function<br> 
						<br> 
						Args:<br> 
						X_train - training variables<br> 
						y_train - training prices<br> 
						<br> 
						Output:<br> 
						model - a trained LSTM model used for predicting y_test<br> 
						'''<br> 
						model = Sequential()<br> 
						#Adding the first LSTM layer and some Dropout regularisation<br> 
						model.add(LSTM(units = 70, return_sequences = True, input_shape = (X_train.shape[1], 10)))<br> 
						model.add(Dropout(0.2))<br> 
						# Adding a second LSTM layer and some Dropout regularisation<br> 
						model.add(LSTM(units = 70, return_sequences = True))<br> 
						model.add(Dropout(0.2))<br> 
						# Adding a third LSTM layer and some Dropout regularisation<br> 
						model.add(LSTM(units = 70, return_sequences = True))<br> 
						model.add(Dropout(0.2))<br> 
						# Adding a fourth LSTM layer and some Dropout regularisation<br> 
						model.add(LSTM(units = 70))<br> 
						model.add(Dropout(0.2))<br> 
						# Adding the output layer<br> 
						model.add(Dense(units = 1))<br> 
						model.add(Activation('relu'))<br> 
						<br> 
						# Compiling the RNN<br> 
						model.compile(optimizer = 'adam', loss = 'mean_squared_error')<br> 
						<br> 
						# Fitting the RNN to the Training set<br> 
						model.fit(X_train, y_train, epochs = 25, batch_size = 32)<br> 
						<br> 
						return model<br> 
					</code>
					<p>Once the model is trained, then stock price predictions are being generated based on the data from the testing dataset.</p>
					<code>	
						X_train, X_test, y_train, y_test, y_test_norm, split_index, days_prior = data_preprocessing(df_GOOG, 21, 0.7)<br>
						model = build_lstm(X_train, y_train)<br>
						predicted_stock_price = model.predict(X_test)<br>
					</code>
					<img src="images\stock-price-5.PNG" alt="stock-price5" style="width: 100%">
					<p>It can be seen that the model has a low loss, indicating that it is doing a good job at guessing the training dataset.</p>
					<p>Once the predictions have been made, they are plotted against the actual values observed in the testing dataset.</p>
					<code>
						# Visualising the results<br>
						plt.figure(figsize=(15,8))<br>
						plt.plot(y_test_norm, color = "red", label = "Real Price - scaled")<br>
						plt.plot(predicted_stock_price, color = "blue", label = "Predicted Price - scaled")<br>
						plt.title('Google Stock Price Prediction - 21-day look-back')<br>
						plt.xlabel('Time range')<br>
						plt.ylabel('Google Stock Price')<br>
						plt.legend()<br>
						plt.show()<br>
						print("Mean absolute error of {0}".format(mean_squared_error(y_test_norm[:-20], predicted_stock_price, squared = False)))<br>
					</code>
					<img src="images\stock-price-6.PNG" alt="stock-price6" style="width: 100%">
					<p>MSE = 0.9545</p>
					<p>Even though the MSE of the model is around 95, it can be seen that the model has done a relatively decent job at predicting the direction at which the stock is going. However, technical indicators have performed well up until the drop in 2020. Afterwards, it can be observed that the technical indicators lose track of the rapid increase of the stock. This serves as proof that there are other factors influencing the rise of the stock — either other technical indicators or external factors like media hype or the phenomenon of fear of missing out etc.</p>
					<p>Refinement</p>
					<p>After observing the values in Figure 11, an attempt has been made to refine the LSTM model by fine tuning the parameters such as the number of layers, the amount of units contained in them, number of epochs etc. Another parameter which has been tweaked with is the number of days preceding the price prediction. The above results have been achieved after looking at 21 days prior to the predicted price. Attempts will be made with a shorter time-frame of 7 days and a longer time-frame of 50 days.</p>
					<p>7 days</p>
					<img src="images\stock-price-7.PNG" alt="stock-price7" style="width: 100%">
					<img src="images\stock-price-8.PNG" alt="stock-price8" style="width: 100%">
					<p>MSE = 1.009</p>
					<p>50 days</p>
					<img src="images\stock-price-9.PNG" alt="stock-price9" style="width: 100%">
					<img src="images\stock-price-10.PNG" alt="stock-price10" style="width: 100%">
					<p>MSE = 0.943</p>
					<p>From the graphs above, it can be seen that the 7-day model follows a similar trend to the 21-day one. However, it has a higher mean square error, making it slightly worse than the 21-day model. The 50-day model on the other hand provides us with a better mean square error than the 21-day. It can, however, be seen that the predicted line does not follow the trend as well as in the 21-day model, lagging behind by a noticeable amount. The lower mean square error could be a result of the blue line having more overlapping parts with the red line, which does not necessarily result in a better model as it can be seen from the figures above.</p>
					<p>Those 3 cases were further tested with different configurations of the LSTM model, including a different number of LSTM layers, dense layers, activation functions and units. However, it did not provide any significant improvements on the ones on display. Out of the activation functions, the relu was outperforming the other available ones by a large margin. Epochs were also increased in size, though it did not result in sizeable differences.</p>
					<h3 style="margin-top: 100px;">Conclusion</h3>
					<p>An attempt was made to predict the next day price of the stock of Google based on its preceding n-days. This was achieved by fetching the data from Yahoo Finances’ API and then applying an LSTM technique in order to take advantage of its ability to manipulate time-series data.</p>
					<p>Apart from the indicators which came with the dataset from Yahoo Finance, further technical indicators have been added in an attempt to increase the accuracy of the predictions. Such additions include the Relative Strength Index (RSI) used as a momentum indicator, the On Balance Volume — used as a volume indicator and the Bollinger Bands — used as a volatility and trend indicators.</p>
					<p>The LSTM model used included 4 LSTM layers with a relu activation function splitting the inputs into 70 features. They were followed by a Dense layer whilst the optimizer that has been used is adam.</p>
					<p>Dataset has been split and converted into tensors at a user defined length which correspond to the number of preceding days the prediction takes into account. Lengths tested include 7-days, 21-days and 50-days. Out of the ones tested the 21-day scenario provided the best results.</p>
					<p>All in all, the model did provide reasonable predictions on the trends of growth and decline, however, it was far from perfect. Furthermore, at the point at which volatility increased, the model did not perform well, not being able to recognize the upward trend of the stock price.</p>
					<p>Improvement</p>
					<p>Improvements on the model include researching and testing out different technical indicators in an attempt to find the ones with the highest predictive power. More combinations within the LSTM model can be explored including layers, number of epochs etc. Furthermore, as mentioned previously, external factors do also influence the change in price of the stock. Incorporating an algorithm which would be able to track the sentiment of the market and applying a weight based on positive or negative news trends might prove to be beneficial in increasing accuracy. A natural language processing algorithm can be used for this purpose to analyze articles or other sources of text information. Once a desirable accuracy is achieved, an algorithm can be written that incorporates a buy/sell strategy which would maximize profits.</p>
					<h3>Useful links:</h3>
					<a href="https://github.com/PWolf96/Stock-Predictor">Github: Predicting stock prices</a><br>
					<a href="https://medium.com/@pv1g15/predicting-stock-prices-afd6781c898e">Medium: Predicting stock prices</a><br>	
				</article>

				<article id="rent-price">
					<h2 class="major">Rent price estimator</h2>
					<h3 style="margin-top: 100px;">Technologies used:</h3>
					<p style="display: flex; align-items: center; max-width: 100%;  flex-wrap: wrap;background-color: rgb(212, 210, 210);margin-top: 20px; margin-bottom: 20px">
						<img src="images\pandas-logo.png" alt="logo1" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\numpy-logo.png" alt="logo2" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\matplotlib-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\scikit-learn-logo.png" alt="logo3" style="width: 120px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\seaborn-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-top: 20px;margin-bottom: 20px" >
						<img src="images\nltk-logo.png" alt="logo3" style="width: 80px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >	
					</p>
					<h3 style="margin-top: 100px;">Project overview</h3>
					<p>For the purpose of this article, I have used AirBnB data for Boston. My aim is to answer 3 questions, namely:</p>
					<p>1. Which factors influence the price of houses the most?</p>
					<p>2. What are the most common words associated with each neighbourhood?</p>
					<p>3. What are the best and worst deals based on the dataset?</p>
					<p>I will do that with the help of the trusty Python and its useful libraries — Pandas, Numpy, Seaborn, Sklearn, Matplotlib, Nltk</p>
					<h3 style="margin-top: 100px;">Getting to know the dataset</h3>
					<p>Before we get into answering the proposed questions, let's have an initial look at the data.</p>
					<img src="images\rent-1.PNG" alt="rent1" style="width: 100%">
					<p>By looking at the price distribution it can be seen that most of the listings in Boston are in the $50–$200 range. But how is that distributed between different neighbourhoods? Figure 2 shows the average prices by neighbourhood. Looks like Bay Village has the highest whilst Mattapan has the lowest.</p>
					<img src="images\rent-2.PNG" alt="rent2" style="width: 100%">
					<p>It is always interesting to understand what makes the price of an accommodation.</p>
					<p>Is it the fact that the property itself is built to a high standard? Or maybe its location provides easier access to places of interest?</p>
					<p>In reality it is a combination of both.</p>
					<h3 style="margin-top: 100px;">Which factors influence the price of houses the most?</h3>
					<p>To answer this question, we will look at the dataset and see how each variable correlates to the price. The dataset already has a data like weekly price, monthly price, number of accommodates, cleaning fee etc. On top of those, the information which contains descriptions of either the accommodation itself, the neighbourhood, the landlord has been analyzed and searched for key words which are specified by us.</p>
					<p>Those key words are:</p>
					<p>‘accessible’, ‘beautiful’, ‘best’, ‘better’, ‘brilliant’, ‘charming’, ‘cheap’, ‘cheerful’, ‘clean’, ‘comfortable’, ‘comfy’, ‘connected’, ‘cosy’, ‘cozy’, ‘deluxe’, ‘desirable’, ‘elegant’, ‘excellent’, ‘favourite’, ‘fine’, ‘good’, ‘gorgeous’, ‘intimate’, ‘large’, ‘nice’, ‘pretty’, ‘private’, ‘spacious’, ‘splendid’, ‘stylish’, ‘tidy’, ‘magnificent’, ‘delicious’, ‘decent’, ‘perfect’, ‘positive’, ‘reasonable’, ‘tremendous’, ‘bright’, ‘modern’, ‘quiet’ , ‘multicultural’, ‘public transportation’, ‘easy access’, ‘shops’, ‘restaurants’, ‘welcoming’, ‘stores’, ‘shop’, ‘store’, ‘gem’, ‘cafes’, ‘transport’, ‘museum’, ‘family’, ‘bars’, ‘roommate’, ‘pet’, ‘dog’, ‘cat’, ‘children’, ‘child’, ‘crazy’, ‘toddler’, ‘kid’, ‘parking’, ‘rail station’, ‘bus station’, ‘private bathroom’, ‘kitchen’, ‘wifi’, ‘home office’, ‘professional’, ‘busy’, ‘chat’, ‘large’, ‘sunny’</p>
					<p>The key words have been selected to match some of the most common phrases used when listing an accommodation.</p>
					<img src="images\rent-3.PNG" alt="rent3" style="width: 100%">
					<img src="images\rent-4.PNG" alt="rent4" style="width: 100%">
					<p>As expected, the highest correlation between the set price is the weekly and monthly price of the accommodation, followed by the number of accommodates it can take. In figure 4, it can be seen that private rooms and 1 beds have the highest inverse correlation with price. It is also worth noting that modern, elegant and deluxe are the ones from the key words that are highly correlated to the price.</p>
					<h3 style="margin-top: 100px;">What are the most common words associated with each neighbourhood?</h3>
					<p>Each listing has a neighbourhood overview section in which the landlord describes the neighbourhood. We will turn our attention to those descriptions in order to answer this question.</p>
					<p>Firstly, we group the descriptions into neighbourhoods. Then we analyze them and count the amount of times each word has been used. The result is a WordCloud visualization which displays the words that are used. The bigger the word is, the more it is used in the descriptions.</p>
					<img src="images\rent-5.PNG" alt="rent5" style="width: 100%">
					<p>Looking at figure 5, Bay Village seems to have a lot of mentions about a park, theater, restaurant. It can be suggested that the higher average prices are driven by a calmer area. Alternatively, Leather District, the second highest average prices neighbourhood, has words like minute walk, station, center etc. which suggests it is a very communicative place.</p>
					<h3 style="margin-top: 100px;">What are the best and worst deals based on the dataset?</h3>
					<p>How can we suggest which deals are good and which are bad? The philosophy behind this idea is that the price a landlord puts on his accommodation does not always correspond to the market in that area. Landlord can have a vague idea of the market but will always have some bias towards their accommodation, swaying the price one way or another.</p>
					<p>When analyzing the whole Boston dataset, a Linear Regression model will be trained to predict the price based on the information it gets from all listings all together — ‘the market’. Then the difference, whether positive or negative, reflects on how good or bad the deal is on renting the place.</p>
					<p>In order to train the model successfully, the model needs to be cleaned of missing values. The factors which have missing values in the dataset are the following:</p>
					<p>‘bathrooms’, ‘beds’, ‘cleaning_fee’, ‘monthly_price’, ‘weekly_price’, ‘review_scores_accuracy’, ‘review_scores_checkin’, ‘review_scores_cleanliness’, ‘review_scores_communication’, ‘review_scores_location’, ‘review_scores_rating’, ‘review_scores_value’, ‘security_deposit’</p>
					<p>Factors such as cleaning fee and security deposit have been considered as services, hence instead of an input representing the cost, it has been transformed into a 1 if the service is present and a 0 if it is not.</p>
					<p>The rest of the missing factors have been populated with their respective means. A more robust approach would include to train a separate model that predicts the monthly price and weekly price based on the other factors. That would increase the accuracy of the prediction. However, it would be left for a future iteration.</p>
					<p>After training the model a result of 0.65% has been achieved with a mean absolute error of 40%. Whilst the performance of the model can be considered low in some cases, in this case it is deemed satisfactory due to the context described earlier.</p>
					<p>In figure 6 the actual price distribution (green line) vs the predicted price distribution (blue line) has been presented in the visualization on the lefthand side. It can be seen from this visualization that there are good deals to be grabbed in the $50–$150 region indicated by the green line being higher than the blue line. On the other hand, it can be seen that the pricier options have not been justified based on the algorithm.</p>
					<p>Figure 7 and figure 8 present the listing ids of the Top 10 best and worst deals. With the best deal being around $200 cheaper than predicted and the worst being $360 more expensive.</p>
					<img src="images\rent-6.PNG" alt="rent6" style="width: 100%">
					<img src="images\rent-7.PNG" alt="rent7" style="width: 100%">
					<img src="images\rent-8.PNG" alt="rent8" style="width: 100%">
					<h3>Useful links:</h3>
					<a href="https://github.com/PWolf96/AirBnB--Boston">Github: Predicting rent prices</a><br>
				</article>

				<article id="disaster-response">
					<h2 class="major">Disaster response</h2>
					<h3 style="margin-top: 100px;">Technologies used:</h3>
					<p style="display: flex; align-items: center; max-width: 100%;  flex-wrap: wrap;background-color: rgb(212, 210, 210);margin-top: 20px; margin-bottom: 20px">
						<img src="images\pandas-logo.png" alt="logo1" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\numpy-logo.png" alt="logo2" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\matplotlib-logo.png" alt="logo3" style="width: 150px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\nltk-logo.png" alt="logo3" style="width: 80px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >	
						<img src="images\sql_logo.png" alt="logo3" style="width: 80px; padding: 0 20px; margin-top: 20px; margin-bottom: 20px" >
						<img src="images\flask_logo.png" alt="logo3" style="width: 80px; padding: 0 20px; margin-top: 20px;margin-bottom: 20px" >
					</p>
					<h3 style="margin-top: 100px;">Project overview</h3>
					<p>This project focuses on analyzing text data. It uses Natural Language Processing to analyze messages and categorize them based on the words used. In this case, it analyzes text responses to a disaster and classifies them so that they are sent to the appropriate response group. The project refers to 2 datasets:</p>
					<p>messages - the messages that were sent</p>
					<p>categories - how those messages have been categorized The process contains, cleaning the data, then creating a machine learning pipeline which is used to train a model to do the aforementioned classification. In the end, a web app will be created where the model will be able to classify messages entered by the user.</p>
					<h3 style="margin-top: 100px;">Process</h3>
					<p>The biggest challenge in the cleaning process was to combine the 2 datasets 'messages' and 'categories' without creating unnecessary columns/rows. To do that messages and categories were cleaned separately. Initially, duplicate values have been removed from the datasets. Then categories has gone through the following cleaning steps:</p>
					<p>
						Split the column "categories" into separate values<br>
						Rename the columns to represent their category name<br>
						Iterate through all values and keep only the numbers. Then convert them to integers<br>
						Adding the id column from the old dataset to the cleaned dataset<br>
					</p>
					<p>Then a new dataframe was created by merging the cleaned versions of 'messages' and 'categories' Finally, the merged dataset was cleaned from columns and rows which contained only 0s as they were deemed as unnecessary for the classification</p>
					<p>Train the classifier<br>
						In order to train the classifierm the following steps needed to be performed:<br>
						<br>
						Load the data from the SQL file<br>
						Create 2 variables: X - containing all the messages; y - containing the dataframe of categories<br>
						Tokenize the messages - each message has been split into words which have been transformed into their root form in lower cases.<br>
						Tokens were created for each word and its respective role in the sentence. All tokens have been added to a list<br>
						A model pipeline has been built with the following elements:<br>
						CountVectorizer - creating the text documents into a matrix of token counts<br>
						TfIdfTransformer - converting the text documents intoa matrix of tf-idf features<br>
						LogisticRegression - classifying the dataset using Logistic regression<br>
					</p>
					<img src="images\nlp-2.PNG" alt="nlp2" style="width: 100%">
					<a href="https://github.com/PWolf96/DisasterResponse-NLP">Github: Disaster response</a><br>
				</article>
		
			</div>
			<div class="container">
				<h2>Featured Project</h2>
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1;">
						<h2>GARVIZ</h2>
						<p>Garviz is a full stack web application which can be used to assist football teams manage their performance.</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<li><a href="#garviz">Read More</a></li>
					</div>
				</div>
			</div>
			<div class="container">
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1; ">
						<h2 style="word-wrap: break-word;">Predicting stock prices</h2>
						<p>This project will explore the possibility of analyzing time-series data in the form of stock prices in order to predict future growth or decline.</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<a href="#stock-price">Read More</a>
					</div>
				</div>
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1;">
						<h2>Disaster response</h2>
						<p>Using natural language processing to categorize messages and send them to the relevant authorities.</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<a href="#disaster-response">Read More</a>
					</div>
				</div>
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1;">
						<h2>Rent price estimator</h2>
						<p>Using linear regression to find the best and worst deals in Boston based on AirBnB data</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<a href="#rent-price">Read More</a>
					</div>
				</div>
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1;">
						<h2>Predicting patterns of play</h2>
						<p>Association analysis is a method used to mine patterns among high volume of events. It groups all the events which happened together and then analyses which occur together more often than not, compared to the total amount of event sequences.</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<a href="#patterns">Read More</a>
					</div>
				</div>
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1;">
						<h2>Player contribution</h2>
						<p>This project makes use of markov chains to evaluate how players contribute to the probability of the team taking a shot on goal</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<a href="#contribution1">Read More</a>
					</div>
				</div>
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1;">
						<h2>Player contribution - 2nd order</h2>
						<p>This project explores further markov chains and tests if accuracy will improve with a higher order analysis</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<a href="#contribution2">Read More</a>
					</div>
				</div>
				<div class="card" style="display: flex; flex-direction: column;">
					<div style="flex-grow: 1;">
						<h2>Predicting playstyles</h2>
						<p>A project which makes use of Long-short term memory to classify a chain of events to a particular playstyle</p>
					</div>
					<div style="align-self: flex-end; flex-grow: 0;">
						<a href="#playstyles">Read More</a>
					</div>
				</div>			
		</div>	
		</div>
		
		<!-- BG -->
		<div id="bg"></div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
	</body>
</html>